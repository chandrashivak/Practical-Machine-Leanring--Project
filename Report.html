<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>Project Report on Practical Machine Learning: </title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>





</head>

<body>
<h1>Project Report on Practical Machine Learning: </h1>

<h3>Problem Statement:</h3>

<p>Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did exercise. This is the &ldquo;classe&rdquo; variable in the training set which needs to be predicted using other variables in the training set.</p>

<h3>Nature of the Data:</h3>

<p>This data has 19622 observations and each observation consists of 160 variables including the outcome variable &ldquo;classe&rdquo;. The variable &ldquo;classe&rdquo; takes values from the set consisting of &ldquo;A&rdquo;, &ldquo;B&rdquo;, &ldquo;C&rdquo;, &ldquo;D&rdquo; and &ldquo;E&rdquo;. The variable &ldquo;classe&rdquo; needs to be predicted from other 159 variables. </p>

<h3>Solution</h3>

<p>The model to predict variable &ldquo;classe&rdquo; is on based random forest method and accuracy of this model is around 97%. Furthermore, our model predicted all 20 test cases correctly.  In the following, we explain all the steps involved in deriving the model. </p>

<ol>
<li><p>The entire data is split into two parts, the first part is 75% of total data for training and the second part is 25% of total data for cross validation. The cross validation data is to predict the accuracy of model. To split the data into two parts randomly, preProcess command with split factor of 0.75 is used.</p></li>
<li><p>We next removed the timestamps-related variables, the variable X and also the user names based on intuition because these parameters don&#39;t influence the outcome. This brings down the number of useful variables to 155.</p></li>
<li><p>Since 155 is a large number of variables to deal with, we first perform a NearZeroVar() operation to remove all the variables with zero or very low variances. In some sense, these variables do not provide much information. After this operation, we were able to bring down the variable count to 100.</p></li>
<li><p>The next important step is to perform a PCA analysis using the preProc() function. By setting a PCA threshold of 0.95, we were able to retain most of the variance with only 36 variables. These 36 variables form our &#39;predictors&#39; and the classe variable is out &#39;outcome&#39;. Given the density of NA variables in the data, we also used the &ldquo;knnImpute&rdquo; function to impute the data.</p></li>
<li><p>We used the train() function to train the data.</p>

<ul>
<li>At first attempt, we only used a generalized linear model (glm) to fit the data. With this, we noticed that the prediction accuracy was as low as 55% on the cross-validation set.</li>
<li>Noting that the &ldquo;glm&rdquo; method works poorly for classification problems, we proceeded to use the random forest (&ldquo;rf&rdquo;) technique of fitting data. In order to shorten the simulation time, we used the following control option on the training set: &#39;trControl=trainControl(method=&ldquo;cv&rdquo;,number= 2)&#39;.  This provided a nice balance between simulation time and accuracy.</li>
</ul></li>
<li><p>The performance of our algorithm as displayed by the confusionMatrix() function is presented below. As established by the accuracy metric, it does very well. The accuracy may be improved (slightly) using a higher PCA threshold and/or tweaking the train function control parameters.  </p></li>
</ol>

<p>The summary of error analysis of cross-validation data is given below. As we can see below, the accuracy of our method is 97% and confidence interval for accuracy is between 96.6% and 97.5%. The following confusion matrix also provides sensitivity and specifity of each case. </p>

<pre><code>Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1373    6    7    8    1
         B   13  915   17    2    2
         C    4   14  829    5    3
         D    4    1   33  759    7
         E    0    4    3    8  886

Overall Statistics

               Accuracy : 0.971          
                 95% CI : (0.966, 0.9756)
    No Information Rate : 0.2843         
    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      

                  Kappa : 0.9634         
 Mcnemar&#39;s Test P-Value : 0.002002       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9849   0.9734   0.9325   0.9706   0.9855
Specificity            0.9937   0.9914   0.9935   0.9891   0.9963
Pos Pred Value         0.9842   0.9642   0.9696   0.9440   0.9834
Neg Pred Value         0.9940   0.9937   0.9852   0.9944   0.9968
Prevalence             0.2843   0.1917   0.1813   0.1595   0.1833
Detection Rate         0.2800   0.1866   0.1690   0.1548   0.1807
Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837
Balanced Accuracy      0.9893   0.9824   0.9630   0.9798   0.9909

</code></pre>

<p>The detailed code used for prediction is given below.</p>

<pre><code>library(caret)
library(kernlab)

# Loading the data
pmldata &lt;- read.csv(&quot;pml-training.csv&quot;)
# Removing irrelevant variables such as name, time stamp, etc
pmldata &lt;- pmldata[, -c(1:5)]

# Splitting data into two parts, one part is for training and another part is for cross-validation
set.seed(32323)
inTrain &lt;- createDataPartition(y=pmldata$classe, p=0.75, list=FALSE)
training &lt;- pmldata[inTrain, ]
testing &lt;- pmldata[-inTrain, ]

# Eliminating near zero variance variables
nsv &lt;- nearZeroVar(training[, ])
training_nzv &lt;- training[, -nsv]

nameind &lt;- which(names(training_nzv)==&quot;user_name&quot;)
ocind &lt;- which(names(training_nzv)==&quot;classe&quot;)

# Applying PCA and removing NA with k neighbor impute method
preProc &lt;- preProcess(training_nzv[, -c(nameind, ocind)], method=c(&quot;knnImpute&quot;, &quot;pca&quot;), thresh=0.95)
trainPC &lt;- predict(preProc, training_nzv[, -c(nameind, ocind)])

# Applying the training based on random forest method
modelFit = train(training$classe~.,method=&quot;rf&quot;,data=trainPC,trControl=trainControl(method=&quot;cv&quot;,number= 2))

# Eliminating near zero variance variables, applying PCA and knn Impute on cross validatin data
testing_nzv &lt;- testing[, -nsv]
testPC &lt;- predict(preProc, testing_nzv[, -c(nameind, ocind)] )
testpredval &lt;- predict(modelFit, testPC)
confusionMatrix(testing$classe, testpredval)

# Predicting the variable classe using test data
pml_testing &lt;- read.csv(&quot;pml-testing.csv&quot;)
pml_testing &lt;- pml_testing[, -c(1:5)]
pml_test_nzv &lt;- pml_testing[, -nsv]
pml_testPC &lt;- predict(preProc, pml_test_nzv[, -c(nameind, ocind)])
pml_testpredval &lt;- predict(modelFit, pml_testPC)

</code></pre>

</body>

</html>

